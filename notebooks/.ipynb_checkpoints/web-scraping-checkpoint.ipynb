{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Web Scraping com Python\n",
    "Rafael Alves Ribeiro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conceito\n",
    "\n",
    "**Web Scraping, ou raspagem de dados**, consiste em um processo que se utiliza de técnicas de programação para a coleta automatizada de dados provenientes da Web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objetivo\n",
    "\n",
    "O principal objetivo do Web Scraping é extrair dados de serviços ou aplicativos que não oferecem uma interface de programação (API) e **transformar dados não estruturados em dados estruturados**\n",
    "\n",
    "<center><img style=\"width: 40%\" src=\"https://www.data-storage.uk/wp-content/uploads/unstructured-data.jpg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scraper\n",
    "\n",
    "Scraper é um software que simula a interação realizada entre um browser operado por um humano e um Web Site. Possui 3 funções básicas:\n",
    "\n",
    "1. Acesso ao Web Site\n",
    "2. Parsing e Extração de conteúdo\n",
    "3. Estruturação dos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Procolo HTTP\n",
    "\n",
    "<div style=\"width: 100%; overflow: hidden;\">\n",
    "    <div style=\"width: 50%; float: left;\">\n",
    "        <center><h3>Requests</h3></center>\n",
    "        <img style=\"width: 100%;\" src=\"https://i0.wp.com/blogs.innovationm.com/wp-content/uploads/2016/10/HTTP-Protocol.png?fit=624%2C248\">\n",
    "    </div>\n",
    "    <div>\n",
    "        <center><h3>Headers</h3></center>\n",
    "        <img style=\"width: 50%;\" src=\"https://mdn.mozillademos.org/files/13687/HTTP_Request.png\">\n",
    "    </div>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Desenvolvendo um Scraper - 6 Etapas\n",
    "  <p></p>\n",
    "<div style=\"width: 100%; overflow: hidden; vertical-align:middle;\">\n",
    "    <div style=\"width: 50%; float: left;\">            \n",
    "    <ol>\n",
    "        <li>Identificar</li>\n",
    "        <li>Navegar</li>\n",
    "        <li>Replicar</li>\n",
    "        <li>Parsear</li>\n",
    "        <li>Validar</li>\n",
    "        <li>Iterar</li>\n",
    "        </ol>\n",
    "    </div>\n",
    "    <div>\n",
    "        <img style=\"width: 50%; vertical-align:middle;\" src=\"https://scrapingpros.com/wp-content/uploads/2017/01/scraper1-1.png\">\n",
    "    </div>\n",
    "</div>\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 1. Identificar\n",
    "\n",
    "No primeiro passo do processo de desenvolvimento de um scraper precisamos entender qual é a estrutura das páginas que queremos raspar e traçar um plano para extrair tudo que precisamos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 2. Navegar\n",
    "\n",
    "Precisamos entender como localizar o dado que queremos\n",
    "extrair dentro do HTML da página. Esse passo pode ser\n",
    "extremamente simples, mas de vez em quando ele se tornará algo bastante complexo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Dev Tools\n",
    "\n",
    "O Dev Tools é conjunto de ferramentas integradas ao browser construı́das para facilitar o desenvolvimento de Web Sites. Permite analisar o código, o tráfego de rede e a performance de uma página. É a principal ferramenta de apoio ao desenvolvimento de Scrapers.\n",
    "<center>\n",
    "<img style=\"width: 40%; float: left;\" src =\"https://cdn.pcsteps.com/wp-content/uploads/2015/09/Reset-Chrome-Reset-Firefox-to-Fix-Most-Problems.png\">\n",
    "<img style=\"width: 25%;\" src =\"https://cdn.instructables.com/FE1/3CR2/GV0KXNWV/FE13CR2GV0KXNWV.LARGE.jpg\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 3. Replicar\n",
    "\n",
    "Neste passo é importante compreender as várias requisições HTTP\n",
    "que a página está realizando para trazer o conteúdo até você,\n",
    "assim poderemos replicar as requisições com nosso Scraper.\n",
    "Utilizaremos a aba Network do Dev Tools para este trabalho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 4. Parsear\n",
    "\n",
    "O anglicismo parsear vem do verbo to parse, que quer dizer algo como analisar ou estudar, mas que, no contexto do Web Scraping, significa extrair os dados desejados de um arquivo HTML. Vejamos um exemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Enviamos uma requisicao HTTP utilizando o método GET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"pt-BR\" class=\"no-js\"  itemscope itemtype=\"https://schema.org/WebPage\">\n",
      "\n",
      "<!-- head -->\n",
      "<head>\n",
      "\n",
      "<!-- meta -->\n",
      "<meta charset=\"UTF-8\" />\n",
      "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1, maximum-scale=1\" />\n",
      "<meta name=\"description\" content=\"Consultoria e Cur\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "HTML = requests.get('http://www.inferir.com.br').text\n",
    "print(HTML[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Realizamos o parse do HTML utilizando BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n",
      "Methods: ['ASCII_SPACES', 'DEFAULT_BUILDER_FEATURES', 'HTML_FORMATTERS', 'NO_PARSER_SPECIFIED_WARNING', 'ROOT_TAG_NAME', 'XML_FORMATTERS', 'append', 'attribselect_re', 'attrs', 'builder', 'can_be_empty_element', 'childGenerator', 'children', 'clear', 'contains_replacement_characters', 'contents', 'currentTag', 'current_data', 'declared_html_encoding', 'decode', 'decode_contents', 'decompose', 'descendants', 'encode', 'encode_contents', 'endData', 'extract', 'fetchNextSiblings', 'fetchParents', 'fetchPrevious', 'fetchPreviousSiblings', 'find', 'findAll', 'findAllNext', 'findAllPrevious', 'findChild', 'findChildren', 'findNext', 'findNextSibling', 'findNextSiblings', 'findParent', 'findParents', 'findPrevious', 'findPreviousSibling', 'findPreviousSiblings', 'find_all', 'find_all_next', 'find_all_previous', 'find_next', 'find_next_sibling', 'find_next_siblings', 'find_parent', 'find_parents', 'find_previous', 'find_previous_sibling', 'find_previous_siblings', 'format_string', 'get', 'getText', 'get_attribute_list', 'get_text', 'handle_data', 'handle_endtag', 'handle_starttag', 'has_attr', 'has_key', 'hidden', 'index', 'insert', 'insert_after', 'insert_before', 'isSelfClosing', 'is_empty_element', 'is_xml', 'known_xml', 'markup', 'name', 'namespace', 'new_string', 'new_tag', 'next', 'nextGenerator', 'nextSibling', 'nextSiblingGenerator', 'next_element', 'next_elements', 'next_sibling', 'next_siblings', 'object_was_parsed', 'original_encoding', 'parent', 'parentGenerator', 'parents', 'parse_only', 'parserClass', 'parser_class', 'popTag', 'prefix', 'preserve_whitespace_tag_stack', 'preserve_whitespace_tags', 'prettify', 'previous', 'previousGenerator', 'previousSibling', 'previousSiblingGenerator', 'previous_element', 'previous_elements', 'previous_sibling', 'previous_siblings', 'pushTag', 'quoted_colon', 'recursiveChildGenerator', 'renderContents', 'replaceWith', 'replaceWithChildren', 'replace_with', 'replace_with_children', 'reset', 'select', 'select_one', 'setup', 'string', 'strings', 'stripped_strings', 'tagStack', 'tag_name_re', 'text', 'unwrap', 'wrap']\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(HTML, 'html.parser')\n",
    "print(type(soup))\n",
    "print('Methods:', [i for i in dir(soup) if not i.startswith('_')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Buscamos a imagem utilizando a tag e a classe encontradas no Dev Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n",
      "<img alt=\"logo-inferir\" class=\"logo-mobile scale-with-grid\" data-height=\"124\" data-retina=\"\" src=\"https://inferir.com.br/wp-content/uploads/2018/08/logo-inferir.png\"/>\n"
     ]
    }
   ],
   "source": [
    "classe_logo = 'logo-mobile scale-with-grid'\n",
    "logo_inferir = soup.find('img', {'class': classe_logo})\n",
    "\n",
    "print(type(logo_inferir))\n",
    "print(logo_inferir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Validar\n",
    "\n",
    "Se tivermos feito tudo certo até agora, validar os resultados será uma tarefa simples. Precisamos apenas reproduzir o procedimento descrito até agora para algumas outras páginas de modo verificar se estamos de fato extraindo corretamente tudo o que queremos.\n",
    "<center>\n",
    "<img style=\"width: 400px;\" src=\"https://cdn-images-1.medium.com/max/475/1*IbHgZrKYCUSeIbL_PywObQ.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. Iterar\n",
    "\n",
    "O último passo consiste em colocar o nosso scraper em produção. Aqui, ele já deve estar funcionando corretamente para todos os casos desejados. Na maior parte dos casos isso consiste em encapsular o scraper em uma função que recebe uma série de links e aplica o mesmo procedimento em cada um. Se quisermos aumentar a eficiência desse processo, podemos paralelizar ou distribuir o nosso raspador.\n",
    "\n",
    "<center>\n",
    "    <img style=\"width: 400px;\" src=\"https://www.freelancinggig.com/blog/wp-content/uploads/2017/02/google-aws-microsoft-azure.jpg\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pacote Requests\n",
    "\n",
    "Requests é um pacote Python que permite o acesso à serviços web, sem a necessidade de conhecimento avançados sobre o protocolo de comunicação HTTP. Empresas como Amazon, Google, Mozilla, PayPal, The Washington Post e Twitter utilizam Requests, que é um dos pacotes Python mais baixados de todos os tempos, com mais de 11 milhões de downloads mensais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RespondentID,Have you seen any of the 6 films in the Star Wars franchise?,Do you consider yourself to be a fan of the Star Wars film franchise?,Which of the following Star Wars films have you seen? Please select all that apply.,,,,,,Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.,,,,,,\"Please state whether you view the following characters favorably, unfavorably, or are unfamiliar with him/her.\",,,,,,,,,,,,,,Which character shot first?,Are you familiar with the Expanded Universe?,Do you consider yourself to be a fan of the Expanded Universe?��,Do you consider yourself to be a fan of the Star Trek franchise?,Gender,Age,Household Income,Education,Location (Census Region)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-139f0880a168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# acessa o texto da resposta enviada pelo servidor, seleciona a primeira linha e imprime na tela\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "import requests\n",
    "#import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/fivethirtyeight/data/master/star-wars-survey/StarWars.csv\"\n",
    "# envia uma requisiçao HTTP utilizando o método GET\n",
    "response = requests.get(url)\n",
    "# acessa o texto da resposta enviada pelo servidor, seleciona a primeira linha e imprime na tela\n",
    "print(response.text.splitlines()[0])\n",
    "pd.read_csv(StringIO(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"https://media.makeameme.org/created/its-your-turn-wj2dce.jpg\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objeto Response\n",
    "\n",
    "Após receber e iterpretar a requisição enviada, o servidor web retorna uma resposta HTTP contendo diversas informações úteis. O Requests realiza o tratamento destes dados e os armazena em um objeto chamado Response.\n",
    "Em nosso exemplo, utilizamos apenas o texto da resposta. Realizaremos uma nova requisição, desta vez solicitando uma imagem, para explorar as informações contidas no objeto Response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__attrs__', '__bool__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__nonzero__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_content', '_content_consumed', '_next', 'apparent_encoding', 'close', 'connection', 'content', 'cookies', 'elapsed', 'encoding', 'headers', 'history', 'is_permanent_redirect', 'is_redirect', 'iter_content', 'iter_lines', 'json', 'links', 'next', 'ok', 'raise_for_status', 'raw', 'reason', 'request', 'status_code', 'text', 'url']\n"
     ]
    }
   ],
   "source": [
    "IMAGE_URL = 'https://media.makeameme.org/created/its-your-turn-wj2dce.jpg'\n",
    "response = requests.get(IMAGE_URL)\n",
    "print(dir(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Enviando Dados - POST\n",
    "\n",
    "* Faça login no site http://testing-ground.scraping.pro/login e inspecione as requisiçoes HTTP no Dev Tools\n",
    "\n",
    "## Replicando o login com o requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "requests.post"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "masterclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
